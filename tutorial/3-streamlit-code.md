# Streamlit(UI) ì½”ë“œ ì„¤ëª…

UI êµ¬í˜„ ë¶€ë¶„ì€ í•µì‹¬ ë‚´ìš©ì€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì£¼ìš” ì½”ë“œ ì„¤ëª…ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.

ì´ë²ˆ App ì—ì„œëŠ” Streamlit ì„ ì´ìš©í•´ì„œ UI ë¥¼ êµ¬í˜„í•˜ê³  ë¹„ì§€ë‹ˆìŠ¤ ë¡œì§ì„ í˜¸ì¶œí•©ë‹ˆë‹¤. Streamlit ì€ ë°ì´í„° ê³¼í•™ìì™€ AI/ML ì—”ì§€ë‹ˆì–´ê°€ ëª‡ ì¤„ì˜ ì½”ë“œë§Œìœ¼ë¡œ ëŒ€í™”í˜• ë°ì´í„° ì•±ì„ ì œê³µí•  ìˆ˜ ìˆëŠ” ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´ì¬ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. 

Streamlit ì—ëŠ” ê³ ìœ í•œ ë°ì´í„° íë¦„ì´ ìˆìŠµë‹ˆë‹¤. í™”ë©´ì—ì„œ ë¬´ì–¸ê°€ë¥¼ ì—…ë°ì´íŠ¸í•´ì•¼ í•  ë•Œë§ˆë‹¤ íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ ì „ì²´ë¥¼ ì²˜ìŒë¶€í„° ëê¹Œì§€ ë‹¤ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ì„œ ì…ë ¥ ìƒìì— í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ë²„íŠ¼ì„ í´ë¦­í•  ë•Œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì²˜ìŒë¶€í„° ì‹¤í–‰í•˜ì—¬ UI ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ì•±ì˜ ì†ŒìŠ¤ ì½”ë“œë¥¼ ìˆ˜ì •í•  ë•Œë„ ë™ì¼í•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤.

ğŸ–¥ï¸ ì „ì²´ ì½”ë“œëŠ” lab/app.py ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## 1. í™”ë©´ì˜ íƒ€ì´í‹€ê³¼ AI ì´ˆê¸° ë©”ì‹œì§€ë¥¼ êµ¬í•©ë‹ˆë‹¤.
![image](https://github.com/user-attachments/assets/b88363f1-eee4-4e19-9f83-cea1fc0caa21)

```python
def set_page_config() -> None:
    """
    Set the Streamlit page configuration.
    """
    st.set_page_config(page_title="ğŸ¤– Chat with Bedrock", layout="wide")
    st.title("ğŸ¤– Chat with Bedrock")
```

```python
def init_chat_data() -> None:
    """
    Reset the chat session and initialize a new conversation chain.
    """
    init_message = {
        "role": "assistant",
        "content": "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” Claude 3 ì±—ë´‡ ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
    }

    st.session_state.messages = []
    st.session_state["langchain_messages"] = []
    st.session_state.messages.append(init_message)
```


### 2. ì™¼ìª½ ë©”ë‰´ì˜ íŒŒë¼ë¯¸í„° ë³€ê²½ / íŒŒì¼ ì—…ë¡œë“œ / Chat ëª¨ë“œ ì„ íƒ í™”ë©´ì„ êµ¬ì„±í•©ë‹ˆë‹¤.

![image](https://github.com/user-attachments/assets/01022c9c-1b78-4fdb-9607-c90692a065e3)

#### 2-1. íŒŒë¼ë¯¸í„° ë³€ê²½ í™”ë©´

```python
def get_sidebar_params() -> Tuple[float, float, int, int, int, str]:
    """
    Get inference parameters from the sidebar.
    """
    with st.sidebar:
        st.markdown("## Inference Parameters")
        model_id_select = st.selectbox(
            'Model',
            ('Claude 3 Sonnet', 'Claude 3 Haiku'),
            key=f"{st.session_state['widget_key']}_Model_Id",
        )

        model_map = {
            "Claude 3 Sonnet": "anthropic.claude-3-sonnet-20240229-v1:0",
            "Claude 3 Haiku": "anthropic.claude-3-haiku-20240307-v1:0"
        }

        model_id = model_map.get(model_id_select)
        temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=1.0,
            value=1.0,
            step=0.1,
            key=f"{st.session_state['widget_key']}_Temperature",
        )

        with st.container():
            col1, col2 = st.columns(2)
            with col1:
                top_p = st.slider(
                    "Top-P",
                    min_value=0.0,
                    max_value=1.0,
                    value=1.00,
                    step=0.01,
                    key=f"{st.session_state['widget_key']}_Top-P",
                )
            with col2:
                top_k = st.slider(
                    "Top-K",
                    min_value=1,
                    max_value=500,
                    value=500,
                    step=5,
                    key=f"{st.session_state['widget_key']}_Top-K",
                )
        with st.container():
            col1, col2 = st.columns(2)
            with col1:
                max_tokens = st.slider(
                    "Max Token",
                    min_value=0,
                    max_value=4096,
                    value=4096,
                    step=8,
                    key=f"{st.session_state['widget_key']}_Max_Token",
                )
            with col2:
                memory_window = st.slider(
                    "Memory Window",
                    min_value=0,
                    max_value=10,
                    value=10,
                    step=1,
                    key=f"{st.session_state['widget_key']}_Memory_Window",
                )

    return temperature, top_p, top_k, max_tokens, memory_window, model_id
```

#### 2-2. íŒŒì¼ ì—…ë¡œë“œ í™”ë©´ (RAG ìš©)

ë¡œì»¬íŒŒì¼ì—ì„œ íŒŒì¼ì„ ì„ íƒí•œ ë’¤ ì—…ë¡œë“œ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ 'create_index_from_pdf_file()' í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.

```python
def set_file_uploader():
    if "uploader_key" not in st.session_state:
        st.session_state.uploader_key = 0

    uploaded_file = st.sidebar.file_uploader("Upload your .pdf file", type={"pdf", "csv"},
                                             key=f"uploader_{st.session_state.uploader_key}")
    if uploaded_file is not None:
        btn = st.sidebar.button("Upload to OpenSearch", type="secondary")
        if btn:
            st.session_state.uploader_key += 1
            os_svc.create_index_from_pdf_file(uploaded_file=uploaded_file)
            st.rerun()
```

#### 2-3. Chat ëª¨ë“œ ì„ íƒ í™”ë©´

```python
def set_mode_selector():
    mode = st.sidebar.radio(
        "Generation Mode",
        [":robot_face: **Normal Chat**",
         ":hourglass_flowing_sand: **History Chat**",
         ":eyeglasses: **RAG Chat**",
         ":bar_chart: **SQL Chat**"],
        index=0,
    )
    return mode
```


### 3. ëŒ€í™” ë‚´ìš©ì„ ì¶œë ¥í•©ë‹ˆë‹¤.

Streamlit ì€ í™”ë©´ ì—…ë°ì´íŠ¸ í•  ë•Œ ë§ˆë‹¤ íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ ì „ì²´ë¥¼ ì²˜ìŒë¶€í„° ëê¹Œì§€ ë‹¤ì‹œ ì‹¤í–‰í•˜ê¸° ë•Œë¬¸ì— ëŒ€í™” ë‚´ìš©ì„ ëª¨ë‘ ì €ì¥í•˜ê³  ìˆë‹¤ê°€ í•œ ë²ˆì— ë³´ì—¬ì¤˜ì•¼ í•©ë‹ˆë‹¤.

![image](https://github.com/user-attachments/assets/36d816af-ffae-4b98-8edb-1228b45bd641)


```python
def display_history_messages() -> None:
    """
    Display chat messages and uploaded images in the Streamlit app.
    """
    for message in st.session_state.messages:
        message_role = message["role"]
        with st.chat_message(message_role):
            message_content = message["content"]
            st.markdown(message_content)
```


### 4. main() : ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ì„œ í™”ë©´ì— ì¶œë ¥í•˜ê³  sessionì— ì €ì¥í•©ë‹ˆë‹¤.

```python
# Get user input message
content = st.chat_input()

# Set new chat button
st.sidebar.button("Start New Chat", on_click=init_chat_data, type="primary")

# Display all history messages
display_history_messages()

# Store user message
if content:
    st.session_state.messages.append({"role": "user", "content": content})
    with st.chat_message("user"):
        st.markdown(content)
```


### 5. main() : ì„ íƒí•œ Chat ëª¨ë“œì— ë”°ë¼ì„œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

```python
# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    # Get response
    with st.chat_message("assistant"):
        if "Normal Chat" in mode:
            response = chat_svc.get_chat_response(model_id=model_id, content=content,
                                                  model_kwargs=model_kwargs)

        if "History Chat" in mode:
            response = chat_svc.get_conversation_chat_response(model_id=model_id, content=content,
                                                               memory_window=memory_window,
                                                               model_kwargs=model_kwargs)
        elif "RAG Chat" in mode:
            response, context = chat_svc.get_rag_chat_response(model_id=model_id, content=content,
                                                               model_kwargs=model_kwargs)
            context = ":memo: ***Context*** :memo: \n\n" + context
            response = response + "\n\n" + context

        elif "SQL Chat" in mode:
            answer, sql_query, sql_result = chat_svc.get_sql_chat_response(model_id=model_id,
                                                                           content=content,
                                                                           model_kwargs=model_kwargs)
            sql_query = ":memo: ***Query*** :memo: \n ``` \n " + sql_query + "\n ```"
            sql_result = ":memo: ***Result*** :memo: \n ``` \n " + sql_result + "\n ```"
            response = answer + "\n\n" + sql_query + "\n\n" + sql_result

    # Store LLM generated responses
    message = {"role": "assistant", "content": response}
    st.session_state.messages.append(message)
    st.rerun()
```